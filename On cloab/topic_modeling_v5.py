# -*- coding: utf-8 -*-
"""Topic_Modeling_V5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12UiWBWTz5qxAubcsdntnc0_84LC1aRla
"""

from google.colab import files
uploaded = files.upload()

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
!pip install -r requirements.txt

import warnings
import nltk
import pyLDAvis
import contractions
import pandas as pd
import re
import matplotlib.pyplot as plt
import unicodedata
from bs4 import BeautifulSoup
import spacy
import seaborn as sns
from wordcloud import WordCloud
from nltk.corpus import stopwords
from google.colab import drive
import opendatasets as od
import plotly.express as px
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation
from spacy.lang.en.stop_words import STOP_WORDS as spacy_stopwords
from bertopic import BERTopic

# For coherence
from gensim.models import CoherenceModel
from gensim.corpora import Dictionary
from tqdm import tqdm

# ===============================================================
# ‚öôÔ∏è 2. ENVIRONMENT SETUP
# ---------------------------------------------------------------
# Downloads NLTK stopwords and lemmatization data,
# loads SpaCy model for NLP tasks,
# and mounts Google Drive (for file access in Colab).
# ===============================================================
warnings.filterwarnings("ignore", category=DeprecationWarning)

warnings.filterwarnings(
    "ignore",
    category=DeprecationWarning,
    message=r"datetime\.datetime\.utcnow\(\) is deprecated"
)


nltk.download("stopwords")
nltk.download("wordnet")
nlp = spacy.load("en_core_web_sm")

# Mount Google Drive (only required in Google Colab)
drive.mount('/content/drive')

# ===============================================================
# üì• 3. LOAD & EXPLORE DATA
# ---------------------------------------------------------------
# Downloads the TripAdvisor Hotel Reviews dataset from Kaggle
# and provides an initial look at review ratings distribution.
# ===============================================================
od.download("https://www.kaggle.com/datasets/andrewmvd/trip-advisor-hotel-reviews")

# Load dataset
raw_data = pd.read_csv('/content/trip-advisor-hotel-reviews/tripadvisor_hotel_reviews.csv')
raw_data['Review'] = raw_data['Review'].str.replace('*', ' stars')
# Copy original data for processing
df = raw_data.copy()

# ===============================================================
# üßπ 4. TEXT CLEANING & PREPROCESSING
# ---------------------------------------------------------------
# Performs comprehensive text cleaning:
# - Remove emojis, HTML tags, URLs, and special symbols
# - Convert text to lowercase
# - Expand contractions like ‚Äúcan‚Äôt‚Äù ‚Üí ‚Äúcannot‚Äù
# - Remove punctuation, numbers, and extra spaces
# ===============================================================

def remove_emoji(text):
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"
        u"\U0001F300-\U0001F5FF"
        u"\U0001F680-\U0001F6FF"
        u"\U0001F1E0-\U0001F1FF"
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251" "+]", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)

# Some rows may be NaN ‚Äî guard against that
df['Review'] = df['Review'].fillna('').astype(str)

df["Review"] = df["Review"].apply(remove_emoji)


def remove_html_tags(text):
    return BeautifulSoup(text, 'html.parser').get_text()

df["Review"] = df["Review"].apply(remove_html_tags)

df["Review"] = df["Review"].str.lower()

def remove_url(text):
    return re.sub(r'https?:\S*', '', text)

def remove_punctuation(text):
    import string
    return ''.join([c for c in text if c not in string.punctuation])

df["Review"] = df["Review"].replace(r'\s+', ' ', regex=True)
df["Review"] = df["Review"].str.replace('<br />', '')

# ===============================================================
# üßΩ 5. TEXT NORMALIZATION, STOPWORD REMOVAL & LEMMATIZATION
# ---------------------------------------------------------------
# - Removes small and numeric tokens
# - Filters stopwords using NLTK list
# - Lemmatizes tokens using SpaCy (e.g., ‚Äúrunning‚Äù ‚Üí ‚Äúrun‚Äù)
# ===============================================================
stop_words = set(stopwords.words("english"))

def clean_text(text):
    text = contractions.fix(text)
    text = re.sub(r"\d+", "", text)
    text = re.sub(r"\b\w{1,2}\b", "", text)
    text = re.sub(r"[^A-Za-z0-9\s]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


def remove_stopwords(text):
    return " ".join([w for w in text.split() if w.lower() not in stop_words])


def lemmatize(text):
    doc = nlp(text)
    return " ".join([token.lemma_ for token in doc])

# Apply cleaning pipeline
df["Review"] = df["Review"].apply(clean_text).apply(remove_stopwords).apply(lemmatize)
print(df.head())

# WordCloud analyzes word frequency and creates a visual.
text = " ".join(df['Review'])
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
plt.figure(figsize=(8, 4))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# ===============================================================

# ===============================================================
# üî† 6. FEATURE EXTRACTION (TF-IDF VECTORIZATION)
# ---------------------------------------------------------------
# Converts text data into a numerical form using TF-IDF (Term Frequency‚ÄìInverse Document Frequency).
# This representation is ideal for algorithms like NMF and SVD
# because it emphasizes unique, informative words while reducing common ones.
# ===============================================================

tfidf_vectorizer = TfidfVectorizer(stop_words=list(spacy_stopwords), min_df=5, max_df=0.7)
tfidf_vectors = tfidf_vectorizer.fit_transform(df["Review"])

# Also prepare CountVectorizer for LDA
count_vectorizer = CountVectorizer(stop_words=list(spacy_stopwords), min_df=5, max_df=0.7)
count_vectors = count_vectorizer.fit_transform(df["Review"])

# ===============================================================
# ‚öñÔ∏è COHERENCE SCORE SETUP
# ---------------------------------------------------------------
# We'll use Gensim's CoherenceModel to evaluate topic coherence.
# Prepare corpus and dictionary for coherence computation
# ===============================================================
texts = [doc.split() for doc in df["Review"]]
dictionary = Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# Helper: display topics (keeps your original function)
def display_topics(h, features, no_top_words=5):
    for topic, words in enumerate(h):
        largest = words.argsort()[::-1]
        print(f"\nTopic {topic}")
        for i in range(no_top_words):
            print("  %s" % (features[largest[i]]))

# ===============================================================
# üß© 7. TOPIC MODELING - NMF (Non-negative Matrix Factorization)
# ---------------------------------------------------------------
# Keep original NMF flow and add coherence-based exploration
# ===============================================================


nmf = NMF(n_components=10, random_state=42, max_iter=600)
W_nmf = nmf.fit_transform(tfidf_vectors)
H_nmf = nmf.components_

def display_topics(h, features, no_top_words=10):
    for topic_idx, topic_weights in enumerate(h):
        print(f"\nTopic {topic_idx + 1}")
        print("-" * 30)
        top_indices = topic_weights.argsort()[::-1][:no_top_words]
        for i in top_indices:
            word = features[i]
            weight = topic_weights[i]
            print(f"{word:<20} | Coefficient: {weight:.4f}")

# Display topics with coefficients
display_topics(H_nmf, tfidf_vectorizer.get_feature_names_out(), no_top_words=10)



# --- Coherence sweep for NMF ---

def compute_coherence_nmf(start=5, stop=50, step=1):
    coherence_scores = []
    topic_range = list(range(start, stop + 1, step))
    for n_topics in tqdm(topic_range, desc='NMF coherence'):
        model_tmp = NMF(n_components=n_topics, random_state=42, max_iter=600)
        W = model_tmp.fit_transform(tfidf_vectors)
        H = model_tmp.components_
        topics = [
            [tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-11:-1]]
            for topic in H
        ]
        cm = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_scores.append(cm.get_coherence())
    plt.plot(topic_range, coherence_scores, marker='o')
    plt.title("NMF Coherence Score by Number of Topics")
    plt.xlabel("Number of Topics")
    plt.ylabel("Coherence Score (c_v)")
    plt.show()
    return pd.DataFrame({'n_topics': topic_range, 'coherence': coherence_scores})

nmf_scores_df = compute_coherence_nmf(5, 50, 1)
print(nmf_scores_df.sort_values('coherence', ascending=False).head())

# ===============================================================
# üß© 8. TOPIC MODELING - SVD (Latent Semantic Analysis / LSA)
# ---------------------------------------------------------------
# Keep original SVD flow and add coherence-based exploration
# ===============================================================


svd = TruncatedSVD(n_components=10, random_state=42)
W_svd = svd.fit_transform(tfidf_vectors)
H_svd = svd.components_

def display_topics_svd(h, features, no_top_words=10):
    for topic_idx, topic_weights in enumerate(h):
        print(f"\nTopic {topic_idx + 1}")
        print("-" * 30)
        top_indices = topic_weights.argsort()[::-1][:no_top_words]
        for i in top_indices:
            word = features[i]
            weight = topic_weights[i]
            print(f"{word:<20} | Coefficient: {weight:.4f}")

# Display SVD topics with coefficients
display_topics_svd(H_svd, tfidf_vectorizer.get_feature_names_out(), no_top_words=10)


# --- Coherence sweep for SVD/LSA ---

def compute_coherence_svd(start=5, stop=50, step=1):
    coherence_scores = []
    topic_range = list(range(start, stop + 1, step))
    for n_topics in tqdm(topic_range, desc='SVD coherence'):
        svd_tmp = TruncatedSVD(n_components=n_topics, random_state=42)
        W = svd_tmp.fit_transform(tfidf_vectors)
        H = svd_tmp.components_
        topics = [
            [tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-11:-1]]
            for topic in H
        ]
        cm = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_scores.append(cm.get_coherence())
    plt.plot(topic_range, coherence_scores, marker='o', color='green')
    plt.title("LSA/SVD Coherence Score by Number of Topics")
    plt.xlabel("Number of Topics")
    plt.ylabel("Coherence Score (c_v)")
    plt.show()
    return pd.DataFrame({'n_topics': topic_range, 'coherence': coherence_scores})

svd_scores_df = compute_coherence_svd(5, 50, 1)
print(svd_scores_df.sort_values('coherence', ascending=False).head())

# ===============================================================
# üß© 9. TOPIC MODELING - LDA (Latent Dirichlet Allocation)
# ---------------------------------------------------------------
# Keep original LDA flow and add coherence-based exploration
# ===============================================================



lda = LatentDirichletAllocation(n_components=10, random_state=42, n_jobs=4)
lda.fit(count_vectors)

# ‚úÖ Display top words with coefficients for each topic
def display_topics_lda(model, feature_names, no_top_words=10):
    for topic_idx, topic_weights in enumerate(model.components_):
        print(f"\nTopic {topic_idx + 1}")
        print("-" * 30)
        top_indices = topic_weights.argsort()[::-1][:no_top_words]
        for i in top_indices:
            word = feature_names[i]
            weight = topic_weights[i]
            print(f"{word:<20} | Coefficient: {weight:.4f}")

display_topics_lda(lda, count_vectorizer.get_feature_names_out(), no_top_words=10)

pyLDAvis.enable_notebook()
vis = pyLDAvis.prepare(
    topic_term_dists=lda.components_ / lda.components_.sum(axis=1)[:, None],
    doc_topic_dists=lda.transform(count_vectors),
    doc_lengths=count_vectors.sum(axis=1).A1,
    vocab=count_vectorizer.get_feature_names_out(),
    term_frequency=count_vectors.sum(axis=0).A1
)

# If running in Jupyter/Colab this will render the visualization
vis

# --- Coherence sweep for LDA ---

def compute_coherence_lda(start=5, stop=50, step=1):
    coherence_scores = []
    topic_range = list(range(start, stop + 1, step))
    for n_topics in tqdm(topic_range, desc='LDA coherence'):
        lda_tmp = LatentDirichletAllocation(n_components=n_topics, random_state=42, n_jobs=-1)
        lda_tmp.fit(count_vectors)
        topics = [
            [count_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-11:-1]]
            for topic in lda_tmp.components_
        ]
        cm = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_scores.append(cm.get_coherence())
    plt.plot(topic_range, coherence_scores, marker='o', color='orange')
    plt.title("LDA Coherence Score by Number of Topics")
    plt.xlabel("Number of Topics")
    plt.ylabel("Coherence Score (c_v)")
    plt.show()
    return pd.DataFrame({'n_topics': topic_range, 'coherence': coherence_scores})

lda_scores_df = compute_coherence_lda(5, 50, 1)
print(lda_scores_df.sort_values('coherence', ascending=False).head())

# ===============================================================
# üß† 10. TOPIC MODELING - BERTopic with Coherence Evaluation (2‚Äì50)
# ---------------------------------------------------------------
# We'll evaluate how coherence changes for different BERTopic
# configurations by varying `min_topic_size` from 2 to 50.
# ===============================================================
from bertopic import BERTopic
from gensim.models import CoherenceModel
from gensim.corpora import Dictionary
from tqdm import tqdm
import matplotlib.pyplot as plt

# üßπ Prepare tokenized texts and dictionary
texts = [str(doc).split() for doc in df["Review"]]  # simple whitespace tokenization
dictionary = Dictionary(texts)

# Define range of valid min_topic_size values (2‚Äì50)
min_topic_sizes = list(range(2, 51))
bertopic_scores = []

for size in tqdm(min_topic_sizes, desc="BERTopic coherence sweep (2‚Äì50)"):
    try:
        # Build and fit model
        model_tmp = BERTopic(
            verbose=False,
            embedding_model="paraphrase-MiniLM-L3-v2",
            min_topic_size=size
        )
        topics_tmp, _ = model_tmp.fit_transform(df["Review"])

        # Extract topic words
        freq_tmp = model_tmp.get_topic_info()
        topic_words = []
        for topic_id in freq_tmp["Topic"]:
            if topic_id == -1:
                continue
            words = [w for w, _ in model_tmp.get_topic(topic_id)]
            topic_words.append(words)

        # Compute coherence if topics are found
        if topic_words:
            cm = CoherenceModel(
                topics=topic_words,
                texts=texts,
                dictionary=dictionary,
                coherence="c_v"
            )
            coherence = cm.get_coherence()
            bertopic_scores.append(coherence)
        else:
            bertopic_scores.append(0)
    except Exception as e:
        print(f"‚ö†Ô∏è Skipped min_topic_size={size} due to error: {e}")
        bertopic_scores.append(0)

# üìà Plot coherence vs. min_topic_size
plt.figure(figsize=(8, 5))
plt.plot(min_topic_sizes, bertopic_scores, marker='o', color='purple')
plt.title("BERTopic Coherence Score by min_topic_size (2‚Äì50)")
plt.xlabel("min_topic_size")
plt.ylabel("Coherence Score (c_v)")
plt.grid(True)
plt.show()

# üßæ Print best configuration
best_size = min_topic_sizes[bertopic_scores.index(max(bertopic_scores))]
print(f"\nüèÜ Best min_topic_size = {best_size} with coherence = {max(bertopic_scores):.4f}")

# üèÅ Display the best configuration
best_idx = int(np.argmax(bertopic_scores))
best_size = min_topic_sizes[best_idx]
best_score = bertopic_scores[best_idx]
print(f"‚úÖ Best BERTopic coherence: {best_score:.4f} (min_topic_size={best_size})")

# üîÅ Train a final BERTopic model using the best parameter

# üîÅ Train a final BERTopic model using the best parameter
model_best = BERTopic(
    verbose=True,
    embedding_model="paraphrase-MiniLM-L3-v2",
    min_topic_size=best_size
)
topics_best, _ = model_best.fit_transform(df["Review"])

# Visualize and inspect

# Get topic information
freq_best = model_best.get_topic_info()

# Exclude the outlier topic (-1)
real_topics = freq_best[freq_best['Topic'] != -1]

# Visualize all real topics in a bar chart
model_best.visualize_barchart(top_n_topics=len(real_topics))

print(len(real_topics))